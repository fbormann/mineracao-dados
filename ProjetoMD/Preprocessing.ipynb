{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5-final"},"colab":{"name":"Preprocessing.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"UIsz3d_vUGRI","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/project')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSOkImYAT_-I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1598572471378,"user_tz":180,"elapsed":3973,"user":{"displayName":"Jose Mauricio Matapi da Silva","photoUrl":"","userId":"09326107684852612879"}},"outputId":"2987bc6f-f9d9-45c7-9a66-6dc1c6f8bcb4"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","# Plotting\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","# Gensim\n","import gensim\n","from gensim.utils import simple_preprocess\n","# NLTK\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet as wn\n","from collections import Counter\n","from wordcloud import WordCloud\n","#Spacy parser\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","import warnings \n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"4JeqmPypT_-f","colab_type":"text"},"source":["## Load data"]},{"cell_type":"code","metadata":{"tags":[],"id":"WB0Ff7vdT_-i","colab_type":"code","colab":{}},"source":["hs = pd.read_csv(\"/content/project/My Drive/Doutorado/2020.1/Mineração de dados/ProjetoMD/Datasets/model_data.csv\")\n","\n","mlma = pd.read_csv(\"/content/project/My Drive/Doutorado/2020.1/Mineração de dados/ProjetoMD/Datasets/mlma_dataset.csv\")\n","\n","hasoc = pd.read_csv(\"/content/project/My Drive/Doutorado/2020.1/Mineração de dados/ProjetoMD/Datasets/hasoc2019_data.csv\",delimiter=\"\\t\")\n","\n","df = pd.concat([hs,mlma, hasoc], axis=0, ignore_index=True)\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T02QTpgST_-1","colab_type":"text"},"source":["## Drop Duplicates"]},{"cell_type":"code","metadata":{"tags":[],"id":"sSbdGDodT_-3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1598453372412,"user_tz":180,"elapsed":918,"user":{"displayName":"Jayr Alencar Pereira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeSJZKTn7UpYO93UjJ44kVykwQWDLwRVpCyhQldw=s64","userId":"06138604672424224858"}},"outputId":"2c9095a9-57bb-468f-915c-9dc429ab4373"},"source":["df.drop_duplicates(subset=['text'], keep='first',inplace=True)\n","df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 10321 entries, 0 to 10500\n","Data columns (total 3 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   Unnamed: 0  10321 non-null  object\n"," 1   text        10321 non-null  object\n"," 2   label       10321 non-null  object\n","dtypes: object(3)\n","memory usage: 322.5+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xjSJ4iCyT__J","colab_type":"text"},"source":["## Descriptive statistics"]},{"cell_type":"code","metadata":{"tags":[],"id":"IXQVTBjwT__L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"error","timestamp":1598572458264,"user_tz":180,"elapsed":1182,"user":{"displayName":"Jose Mauricio Matapi da Silva","photoUrl":"","userId":"09326107684852612879"}},"outputId":"1ba5bd42-badf-4e10-b863-a1e2c541d9dc"},"source":["count = df['text'].str.split().str.len()\n","count.index = count.index.astype(str) + ' words:'\n","print(\"Total number of words:\", count.sum(), \"words\")\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-33ba2b3b582e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' words:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of words:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","metadata":{"tags":[],"id":"uWFK7uSqT__Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598453378629,"user_tz":180,"elapsed":1172,"user":{"displayName":"Jayr Alencar Pereira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeSJZKTn7UpYO93UjJ44kVykwQWDLwRVpCyhQldw=s64","userId":"06138604672424224858"}},"outputId":"0470544c-b3ce-4b77-a748-3321bb86d4d5"},"source":["print(\"Mean number of words per tweet:\", round(count.mean(), 2),'words')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mean number of words per tweet: 12.31 words\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"tags":[],"id":"V2agCt8hT__o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598453378927,"user_tz":180,"elapsed":803,"user":{"displayName":"Jayr Alencar Pereira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeSJZKTn7UpYO93UjJ44kVykwQWDLwRVpCyhQldw=s64","userId":"06138604672424224858"}},"outputId":"aab6a655-ecfa-491c-a164-b960a6e115d2"},"source":["df['tweet_length'] = df['text'].str.len()\n","print(\"Total length of the dataset is:\",df.tweet_length.sum(), \"characters\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total length of the dataset is: 781347 characters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"tags":[],"id":"6uE32YBDT__0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598453380522,"user_tz":180,"elapsed":764,"user":{"displayName":"Jayr Alencar Pereira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeSJZKTn7UpYO93UjJ44kVykwQWDLwRVpCyhQldw=s64","userId":"06138604672424224858"}},"outputId":"8ebdee34-58bd-4d6f-c164-ac78f41ce339"},"source":["print(\"Mean length of a tweet is:\", round(df.tweet_length.mean(),0),'characters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mean length of a tweet is: 76.0 characters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q338S6sYUAAA","colab_type":"text"},"source":["# Feature engineering (before exclusion of a few important data)"]},{"cell_type":"code","metadata":{"id":"fIzlrM1MUAAB","colab_type":"code","colab":{}},"source":["def has_element(tweet, pattern1, pattern2):\n","    r = re.findall(pattern1, tweet)\n","    new_tweet = None\n","    for i in r:\n","        new_tweet = re.sub(i, '', tweet)\n","    \n","    if new_tweet and new_tweet != tweet:\n","        return True\n","    r = re.findall(pattern2, tweet)\n","    for i in r:\n","        new_tweet = re.sub(i, '', tweet)\n","    if new_tweet and new_tweet != tweet:\n","        return True\n","    return False\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2Fot2spUAAN","colab_type":"code","colab":{}},"source":["df[\"has_user\"] = np.vectorize(has_element)(df['text'], \"@ [\\w]*\", \"@[\\w]*\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V8cFyFGXUAAW","colab_type":"code","colab":{}},"source":["df[\"has_hashtag\"] = np.vectorize(has_element)(df['text'], \"# [\\w]*\", \"#[\\w]*\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2B-GB57oUAAf","colab_type":"code","colab":{}},"source":["df[\"has_url\"] = np.vectorize(has_element)(df['text'], r\"http\\S+\", r\"http\\S+\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AuyY6T0RUAAo","colab_type":"text"},"source":["## Remove @users\n","Twitter enables including usernames within tweets through the symbol “@.” These do not possess any value for our analysis; hence they are removed from the dataset using a function."]},{"cell_type":"code","metadata":{"id":"F7EeA0gyUAAp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"ok","timestamp":1598453385717,"user_tz":180,"elapsed":678,"user":{"displayName":"Jayr Alencar Pereira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeSJZKTn7UpYO93UjJ44kVykwQWDLwRVpCyhQldw=s64","userId":"06138604672424224858"}},"outputId":"0d844938-d01e-4bb7-a031-c3c91bf28608"},"source":["def remove_users(tweet, pattern1, pattern2):\n","    r = re.findall(pattern1, tweet)\n","    for i in r:\n","        tweet = re.sub(i, '', tweet)\n","  \n","    r = re.findall(pattern2, tweet)\n","    for i in r:\n","        tweet = re.sub(i, '', tweet)\n","    return tweet\n","df['tidy_tweet'] = np.vectorize(remove_users)(df['text'],     \"@ [\\w]*\", \"@[\\w]*\")\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>tweet_length</th>\n","      <th>has_user</th>\n","      <th>has_hashtag</th>\n","      <th>has_url</th>\n","      <th>tidy_tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>These girls are the equivalent of the irritati...</td>\n","      <td>racism</td>\n","      <td>99</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>These girls are the equivalent of the irritati...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Who is writing the bimbolines? #mkr</td>\n","      <td>sexism</td>\n","      <td>35</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Who is writing the bimbolines? #mkr</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Colin will save them. They're pretty blondes, ...</td>\n","      <td>sexism</td>\n","      <td>75</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Colin will save them. They're pretty blondes, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Which will end first: #mkr or Tony Abbott as PM?</td>\n","      <td>none</td>\n","      <td>48</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Which will end first: #mkr or Tony Abbott as PM?</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>RT @TheAngelaOfOz: That's bullshit Colin and y...</td>\n","      <td>none</td>\n","      <td>62</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>RT : That's bullshit Colin and you know it. #mkr</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Unnamed: 0  ...                                         tidy_tweet\n","0          0  ...  These girls are the equivalent of the irritati...\n","1          1  ...                Who is writing the bimbolines? #mkr\n","2          2  ...  Colin will save them. They're pretty blondes, ...\n","3          3  ...   Which will end first: #mkr or Tony Abbott as PM?\n","4          4  ...   RT : That's bullshit Colin and you know it. #mkr\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"WCBa7vAdUAA1","colab_type":"text"},"source":["## lowercase normalization"]},{"cell_type":"code","metadata":{"id":"wtqyoAP2UAA3","colab_type":"code","colab":{}},"source":["df['tidy_tweet'] = df['tidy_tweet'].str.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAU9GVLyUAA-","colab_type":"text"},"source":["## Remove hashtags (#)\n","Same as with usernames, hashtags also are considered not of significant value for topic modeling analysis, in particular, therefore, are removed.\n","\n","** seria bom ver quais as hashtags mais frequentes **"]},{"cell_type":"code","metadata":{"id":"fnSizN4tUABA","colab_type":"code","colab":{}},"source":["df['tidy_tweet'] = np.vectorize(remove_users)(df['tidy_tweet'], \"# [\\w]*\", \"#[\\w]*\")\n","df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7iQa2ZBUABL","colab_type":"text"},"source":["## Remove links\n"]},{"cell_type":"code","metadata":{"id":"ked-hNYDUABN","colab_type":"code","colab":{}},"source":["def remove_links(tweet):\n","    tweet_no_link = re.sub(r\"http\\S+\", \"\", tweet)\n","    return tweet_no_link\n","df['tidy_tweet'] = np.vectorize(remove_links)(df['tidy_tweet'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QzTWlQUjUABU","colab_type":"text"},"source":["## remove punctuations, numbers, special characters and short words"]},{"cell_type":"code","metadata":{"id":"RvrwtoWjUABV","colab_type":"code","colab":{}},"source":["# REMOVE Punctuations, Numbers, and Special Characters\n","df['tidy_tweet'] = df['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n","\n","# REMOVE SHORT WORDS\n","df['tidy_tweet'] = df['tidy_tweet'].apply(lambda x:' '.join([w for w in x.split() if len(w)>3]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vLSkLJxeUABc","colab_type":"text"},"source":["## words replaces"]},{"cell_type":"code","metadata":{"id":"97kQ7CiiUABd","colab_type":"code","colab":{}},"source":["# retard and retarded\n","df['tidy_tweet'] = df['tidy_tweet'].str.replace(\"retarded\", \"retard\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7s5syKSRUABk","colab_type":"text"},"source":["## Lemmatization\n","An essential step of pre-processing is known as Tokenization. It is the process where the text is split according to whitespaces, and every word and punctuation is saved as a separate token. We perform this step by using spacy parser.\n","\n","Proper nouns are also removed at this point\n"]},{"cell_type":"code","metadata":{"tags":[],"id":"QFooJ7JEUABl","colab_type":"code","colab":{}},"source":["def tokenize_spacy(tweet):\n","    for text in tweet:\n","        doc = nlp(text)\n","        # filtered_sentence = [t.lemma_ for t in doc if t.pos_ not in [\"PUNCT\",'PROPN',\"PRON\"]] # remove proper nouns and pronouns\n","        filtered_sentence = [t.lemma_+\"|\"+t.pos_ for t in doc if t.pos_ not in [\"PUNCT\",'PROPN',\"PRON\",\"DET\"]] # remove proper nouns and pronouns\n","        # pe\n","        yield(filtered_sentence) \n","\n","df['tidy_tweet_tokens'] = list(tokenize_spacy(df['tidy_tweet']))\n","df.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HCPwWBF4UABt","colab_type":"text"},"source":["## Remove stopwords\n","\n","Next, we remove stopwords that have no analytic value, usually articles, prepositions, or pronouns, for instance, ‘a,’ ‘and,’ ‘the,’ etc. The default list can be adjusted and extended as desired. We added some new words to the predefined list of Natural Language Toolkit (NLTK), which contains 179 words."]},{"cell_type":"code","metadata":{"id":"nnc_z4jFUABw","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","# Prepare Stop Words\n","\n","stop_words = stopwords.words('english')\n","stop_words.extend([ 'https', 'twitter', 'rt', 'pic','twitt','amp','pron'])# REMOVE STOPWORDS\n","\n","def remove_stopwords(tweets):\n","    return [[word for word in tweet if word.split(\"|\")[0] not in stop_words] for tweet in tweets]\n","\n","df['tokens_no_stop'] = remove_stopwords(df['tidy_tweet_tokens'])\n","df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gT5y5R64UAB3","colab_type":"text"},"source":["## REMOVE TWEETS LESS THAN 3 TOKENS\n","For topic modeling and also sentiment analysis, documents with less than three tokens are not suitable to generate enough information."]},{"cell_type":"code","metadata":{"tags":[],"id":"DPKdjbZQUAB4","colab_type":"code","colab":{}},"source":["df['length'] = df['tokens_no_stop'].apply(len)\n","df = df.drop(df[df['length']<3].index)\n","df = df.drop(['length'], axis=1)\n","df.shape\n","df.reset_index(drop=True, inplace=True)\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ouNdgXglUACB","colab_type":"text"},"source":["## Word Clouds\n","### All tweets"]},{"cell_type":"code","metadata":{"tags":[],"id":"CYwF3bbcUACC","colab_type":"code","colab":{}},"source":["flat_list = [item for sublist in df['tokens_no_stop'].values for item in sublist]\n","\n","wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(Counter(flat_list))\n","\n","plt.figure( figsize=(20,10), facecolor='k')\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ys0KyGLEUACL","colab_type":"text"},"source":["### Offensive\n"]},{"cell_type":"code","metadata":{"tags":[],"id":"OdVvnnK6UACP","colab_type":"code","colab":{}},"source":["offensive_df = df[~df.label.isin(['none','normal','NOT'])]\n","flat_list = [item for sublist in offensive_df['tokens_no_stop'].values for item in sublist]\n","\n","wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(Counter(flat_list))\n","\n","plt.figure( figsize=(20,10), facecolor='k')\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","offensive_df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wq2rI91eUACV","colab_type":"text"},"source":["## Not Offensive\n"]},{"cell_type":"code","metadata":{"tags":[],"id":"OAnJnjfQUACX","colab_type":"code","colab":{}},"source":["offensive_df = df[df.label.isin(['none','normal','NOT'])]\n","flat_list = [item for sublist in offensive_df['tokens_no_stop'].values for item in sublist]\n","\n","wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(Counter(flat_list))\n","\n","plt.figure( figsize=(20,10), facecolor='k')\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","offensive_df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36xXuEqzUACf","colab_type":"code","colab":{}},"source":["df.to_pickle(\"/content/project/My Drive/Doutorado/2020.1/Mineração de dados/ProjetoMD/preprocessed_pos\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"p74dQ5ZGUACl","colab_type":"code","colab":{}},"source":["flat_list = [item for sublist in df['tokens_no_stop'].values for item in sublist]\n","c = Counter(flat_list)\n","c.most_common(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kchZcUiuUACv","colab_type":"code","colab":{}},"source":["df[['text','tokens_no_stop']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibeot0U-NXW9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1599912399393,"user_tz":180,"elapsed":2174,"user":{"displayName":"Jayr Alencar Pereira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeSJZKTn7UpYO93UjJ44kVykwQWDLwRVpCyhQldw=s64","userId":"06138604672424224858"}},"outputId":"3568ce79-2405-40f8-b892-b341f9e017dd"},"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","print(stopwords.words(\"english\"))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"],"name":"stdout"}]}]}